{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sesion6_MDP_Grid_Pathfinding.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MXwFDyxM1rQ8"},"source":["# Diplomatura de Especialización en Desarrollo de Aplicaciones con Inteligencia Artificial - Inteligencia Artificial para Juegos (Game AI) - Sesión 6 (Ejemplo)"]},{"cell_type":"markdown","metadata":{"id":"HWV9A-zMtpFg"},"source":["<font color='orange'>Entorno de Grilla MDP (Laberinto *Pathfinding Problem*) con visualizador de iteración de valor. Tarea: Completar funciones de iteración de valor.</font>"]},{"cell_type":"markdown","metadata":{"id":"Djld92hzs1GF"},"source":["Códigos adaptados del repositorio [aima-python](https://github.com/aimacode/aima-python)."]},{"cell_type":"markdown","metadata":{"id":"l0Z66_u3ANay"},"source":["## MDP: Definition"]},{"cell_type":"code","metadata":{"id":"PfSZuU5fEudi"},"source":["orientations = EAST, NORTH, WEST, SOUTH = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n","turns = LEFT, RIGHT = (+1, -1)\n","import operator\n","def vector_add(a, b):\n","    \"\"\"Component-wise addition of two vectors.\"\"\"\n","    return tuple(map(operator.add, a, b))\n","\n","def turn_heading(heading, inc, headings=orientations):\n","    return headings[(headings.index(heading) + inc) % len(headings)]\n","\n","def turn_right(heading):\n","    return turn_heading(heading, RIGHT)\n","\n","\n","def turn_left(heading):\n","    return turn_heading(heading, LEFT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w_2M3moXANaz"},"source":["class MDP:\n","\n","    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n","    and reward function. We also keep track of a gamma value, for use by\n","    algorithms. The transition model is represented somewhat differently from\n","    the text. Instead of P(s' | s, a) being a probability number for each\n","    state/state/action triplet, we instead have T(s, a) return a\n","    list of (p, s') pairs. We also keep track of the possible states,\n","    terminal states, and actions for each state. [page 646]\"\"\"\n","\n","    def __init__(self, init, actlist, terminals, transitions = {}, reward = None, states=None, gamma=.9):\n","        if not (0 < gamma <= 1):\n","            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n","\n","        if states:\n","            self.states = states\n","        else:\n","            ## collect states from transitions table\n","            self.states = self.get_states_from_transitions(transitions)\n","            \n","        \n","        self.init = init\n","        \n","        if isinstance(actlist, list):\n","            ## if actlist is a list, all states have the same actions\n","            self.actlist = actlist\n","        elif isinstance(actlist, dict):\n","            ## if actlist is a dict, different actions for each state\n","            self.actlist = actlist\n","        \n","        self.terminals = terminals\n","        self.transitions = transitions\n","        if self.transitions == {}:\n","            print(\"Warning: Transition table is empty.\")\n","        self.gamma = gamma\n","        if reward:\n","            self.reward = reward\n","        else:\n","            self.reward = {s : 0 for s in self.states}\n","        #self.check_consistency()\n","\n","    def R(self, state):\n","        \"\"\"Return a numeric reward for this state.\"\"\"\n","        return self.reward[state]\n","\n","    def T(self, state, action):\n","        \"\"\"Transition model. From a state and an action, return a list\n","        of (probability, result-state) pairs.\"\"\"\n","        if(self.transitions == {}):\n","            raise ValueError(\"Transition model is missing\")\n","        else:\n","            return self.transitions[state][action]\n","\n","    def actions(self, state):\n","        \"\"\"Set of actions that can be performed in this state. By default, a\n","        fixed list of actions, except for terminal states. Override this\n","        method if you need to specialize by state.\"\"\"\n","        if state in self.terminals:\n","            return [None]\n","        else:\n","            return self.actlist\n","\n","    def get_states_from_transitions(self, transitions):\n","        if isinstance(transitions, dict):\n","            s1 = set(transitions.keys())\n","            s2 = set([tr[1] for actions in transitions.values() \n","                              for effects in actions.values() for tr in effects])\n","            return s1.union(s2)\n","        else:\n","            print('Could not retrieve states from transitions')\n","            return None\n","\n","    def check_consistency(self):\n","        # check that all states in transitions are valid\n","        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n","        # check that init is a valid state\n","        assert self.init in self.states\n","        # check reward for each state\n","        #assert set(self.reward.keys()) == set(self.states)\n","        assert set(self.reward.keys()) == set(self.states)\n","        # check that all terminals are valid states\n","        assert all([t in self.states for t in self.terminals])\n","        # check that probability distributions for all actions sum to 1\n","        for s1, actions in self.transitions.items():\n","            for a in actions.keys():\n","                s = 0\n","                for o in actions[a]:\n","                    s += o[0]\n","                assert abs(s - 1) < 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFcDeWg7ANa2"},"source":["The **_ _init_ _** method takes in the following parameters:\n","\n","- init: the initial state.\n","- actlist: List of actions possible in each state.\n","- terminals: List of terminal states where only possible action is exit\n","- gamma: Discounting factor. This makes sure that delayed rewards have less value compared to immediate ones.\n","\n","**R** method returns the reward for each state by using the self.reward dict.\n","\n","**T** method is not implemented and is somewhat different from the text. Here we return (probability, s') pairs where s' belongs to list of possible state by taking action a in state s.\n","\n","**actions** method returns list of actions possible in each state. By default it returns all actions for states other than terminal states.\n"]},{"cell_type":"markdown","metadata":{"id":"0bLV07NnANbA"},"source":["## GRID MDP\n","\n","Now we look at a concrete implementation that makes use of the MDP as base class. The GridMDP class in the mdp module is used to represent a grid world MDP like the one shown in  in **Fig 17.1** of the AIMA Book.\n","\n","We assume for now that the environment is _fully observable_, so that the agent always knows where it is. The code should be easy to understand if you have gone through the CustomMDP example."]},{"cell_type":"code","metadata":{"id":"k4wUicDDE8Ra"},"source":["def redondea(num, cant_decimales):\n","  if(num==None):\n","    return None\n","  else:\n","    return round(num, cant_decimales)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHcewSF9ANbB"},"source":["class GridMDP(MDP):\n","\n","    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1]. All you have to do is\n","    specify the grid as a list of lists of rewards; use None for an obstacle\n","    (unreachable state). Also, you should specify the terminal states.\n","    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n","\n","    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n","        grid.reverse()  # because we want row 0 on bottom, not on top\n","        reward = {}\n","        states = set()\n","        self.rows = len(grid)\n","        self.cols = len(grid[0])\n","        self.grid = grid\n","        for x in range(self.cols):\n","            for y in range(self.rows):\n","                if grid[y][x] is not None:\n","                    states.add((x, y))\n","                    reward[(x, y)] = grid[y][x]\n","        self.states = states\n","        actlist = orientations\n","        transitions = {}\n","        for s in states:\n","            transitions[s] = {}\n","            for a in actlist:\n","                transitions[s][a] = self.calculate_T(s, a)\n","        MDP.__init__(self, init, actlist=actlist,\n","                     terminals=terminals, transitions = transitions, \n","                     reward = reward, states = states, gamma=gamma)\n","\n","    def calculate_T(self, state, action):\n","        if action is None:\n","            return [(0.0, state)]\n","        else:\n","            return [(0.8, self.go(state, action)),\n","                    (0.1, self.go(state, turn_right(action))),\n","                    (0.1, self.go(state, turn_left(action)))]\n","    \n","    def T(self, state, action):\n","        if action is None:\n","            return [(0.0, state)]\n","        else:\n","            return self.transitions[state][action]\n"," \n","    def go(self, state, direction):\n","        \"\"\"Return the state that results from going in this direction.\"\"\"\n","        state1 = vector_add(state, direction)\n","        return state1 if state1 in self.states else state\n","\n","    def to_grid(self, mapping):\n","        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n","        return list(reversed([[redondea(mapping.get((x, y)),2)\n","                               for x in range(self.cols)]\n","                              for y in range(self.rows)]))\n","\n","    def toGrid(self, mapping):\n","        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n","        return list(reversed([[mapping.get((x, y), None)\n","                               for x in range(self.cols)]\n","                              for y in range(self.rows)]))\n","\n","    def to_arrows(self, policy):\n","        chars = {\n","            (1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n","        return self.toGrid({s: chars[a] for (s, a) in policy.items()})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fxas04oMANbE"},"source":["The **_ _init_ _** method takes **grid** as an extra parameter compared to the MDP class. The grid is a nested list of rewards in states.\n","\n","**go** method returns the state by going in particular direction by using vector_add.\n","\n","**T** method is not implemented and is somewhat different from the text. Here we return (probability, s') pairs where s' belongs to list of possible state by taking action a in state s.\n","\n","**actions** method returns list of actions possible in each state. By default it returns all actions for states other than terminal states.\n","\n","**to_arrows** are used for representing the policy in a grid like format."]},{"cell_type":"markdown","metadata":{"id":"dmNEz7vqANbF"},"source":["We can create a GridMDP like the one in **Fig 17.1** as follows: \n","\n","    GridMDP([[-0.04, -0.04, -0.04, +1],\n","            [-0.04, None,  -0.04, -1],\n","            [-0.04, -0.04, -0.04, -0.04]],\n","            terminals=[(3, 2), (3, 1)])\n","\n","<img src='https://raw.githubusercontent.com/iapucp/IA-Diplomado/master/Inteligencia%20Artificial%20para%20Juegos/Codigo_Sesion6/fig171_aima.png' width=500px>\n","            \n","In fact the **sequential_decision_environment** in mdp module has been instantized using the exact same code."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"e3QrUG4zDcz-","outputId":"7148fac2-8b18-4635-8449-e241ed94bd7d"},"source":["GridMDP([[-0.04, -0.04, -0.04, +1],\n","        [-0.04, None,  -0.04, -1],\n","        [-0.04, -0.04, -0.04, -0.04]],\n","        terminals=[(3, 2), (3, 1)])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.GridMDP at 0x274e75d43c8>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"hNY366xjFTeW"},"source":["\n","\"\"\" [Figure 17.1]\n","A 4x3 grid environment that presents the agent with a sequential decision problem.\n","\"\"\"\n","\n","sequential_decision_environment = GridMDP([[-0.04, -0.04, -0.04, +1],\n","                                           [-0.04, None, -0.04, -1],\n","                                           [-0.04, -0.04, -0.04, -0.04]],\n","                                          terminals=[(3, 2), (3, 1)])\n","\n","\n","# ______________________________________________________________________________"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"zcurkNPiANbG","outputId":"29e753f0-ebcd-47bc-dc10-1ae9ee382e1d"},"source":["sequential_decision_environment"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.GridMDP at 0x274e75d4710>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"L9TeYRFICeyO","outputId":"fb384176-b248-4cd0-c37b-2b89b4c53869"},"source":["sequential_decision_environment.grid"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[-0.04, -0.04, -0.04, -0.04],\n"," [-0.04, None, -0.04, -1],\n"," [-0.04, -0.04, -0.04, 1]]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"fQL4xqoItPm8"},"source":["## GRID MDP Pathfinding Problem\n","Markov Decision Processes can be used to find the best path through a maze. Let us consider this simple maze.\n","![title](https://github.com/aimacode/aima-python/blob/master/images/maze.png?raw=1)\n","\n","This environment can be formulated as a GridMDP.\n","<br>\n","To make the grid matrix, we will consider the state-reward to be -0.1 for every state.\n","<br>\n","State (1, 1) will have a reward of -5 to signify that this state is to be prohibited.\n","<br>\n","State (9, 9) will have a reward of +5.\n","This will be the terminal state.\n","<br>\n","The matrix can be generated using the GridMDP editor or we can write it ourselves."]},{"cell_type":"code","metadata":{"id":"j21BdJ6F7jdw"},"source":["gridXD = [\n","    [None, None, None, None, None, None, None, None, None, None, None], \n","    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None, +5.0, None], \n","    [None, -0.1, None, None, None, None, None, None, None, -0.1, None], \n","    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None], \n","    [None, -0.1, None, None, None, None, None, None, None, None, None], \n","    [None, -0.1, None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None], \n","    [None, -0.1, None, None, None, None, None, -0.1, None, -0.1, None], \n","    [None, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, None, -0.1, None], \n","    [None, None, None, None, None, -0.1, None, -0.1, None, -0.1, None], \n","    [None, -5.0, -0.1, -0.1, -0.1, -0.1, None, -0.1, None, -0.1, None], \n","    [None, None, None, None, None, None, None, None, None, None, None]\n","] "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"KFVZdKL49rIx","outputId":"a5939c80-2529-4fe9-96d7-eb9830f68015"},"source":["len(gridXD)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"KfcVx19XtPm-"},"source":["We have only one terminal state, (9, 9)"]},{"cell_type":"code","metadata":{"id":"7TxCo5H6a-6_"},"source":["terminals = [(9, 9)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GoW3FdDtbaHW"},"source":["We define our maze environment below"]},{"cell_type":"code","metadata":{"id":"qO_O7YPHbaHa"},"source":["maze = GridMDP(gridXD, terminals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BDC0dPqMbaHe"},"source":["To solve the maze, we can use the `best_policy` function along with `value_iteration`."]},{"cell_type":"code","metadata":{"id":"-mDMZKYKYQyC"},"source":["argmax=max\n","def value_iteration(mdp, epsilon=0.001): epsilon: tolerancia de error.\n","    U1 = {s: 0 for s in mdp.states}\n","    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n","    while True:\n","        U = U1.copy()\n","        delta = 0\n","        for s in mdp.states:\n","            U1[s] = R(s) + gamma * max(sum(p * U[s1] for (p, s1) in T(s, a))\n","                                       for a in mdp.actions(s))\n","            delta = max(delta, abs(U1[s] - U[s]))\n","        if delta <= epsilon * (1 - gamma) / gamma:\n","            return U\n","\n","def best_policy(mdp, U):\n","    \"\"\"Given an MDP and a utility function U, determine the best policy,\n","    as a mapping from state to action. (Equation 17.4)\"\"\"\n","\n","    pi = {}\n","    for s in mdp.states:\n","        pi[s] = argmax(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n","    return pi\n","\n","\n","def expected_utility(a, s, U, mdp):\n","    \"\"\"The expected utility of doing a in state s, according to the MDP and U.\"\"\"\n","\n","    return sum(p * U[s1] for (p, s1) in mdp.T(s, a))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"WVDQfcq7baHf","outputId":"d8dc03c2-4dd4-4d64-bac6-b0bc7e15f22c"},"source":["maze"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.GridMDP at 0x274e75d4748>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"7N9FzQ06baHj"},"source":["uu = value_iteration(maze) #resuelve las utilidades de maze\r\n","#es decir, ejecuta el value iteration "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGjyO_fCbaHm","outputId":"d12567e2-0a6c-4a7c-ec17-7e4faf67e81f"},"source":["uu"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(5, 9): -0.28972320275406793,\n"," (4, 7): 1.3823597016890337,\n"," (1, 3): -0.07872276426135358,\n"," (6, 9): -0.37634232586787497,\n"," (7, 3): -0.5942537706997953,\n"," (9, 1): -0.8623821312777787,\n"," (9, 8): 4.2682926829268295,\n"," (7, 7): 2.5192603952355515,\n"," (2, 1): -0.72170380254087,\n"," (1, 6): 0.37914151714702293,\n"," (9, 4): -0.7965058489600694,\n"," (3, 7): 1.0918280307513455,\n"," (5, 1): -0.5888945373708313,\n"," (8, 5): -0.7324958860867973,\n"," (7, 2): -0.6437352855643286,\n"," (4, 9): -0.1910736469037466,\n"," (3, 3): -0.28972320275406793,\n"," (2, 9): 0.04923240747471591,\n"," (5, 5): -0.7651234106697301,\n"," (6, 7): 2.090082298255606,\n"," (6, 3): -0.525529612393064,\n"," (1, 5): 0.21095352724371716,\n"," (4, 1): -0.6390295013376613,\n"," (1, 1): -6.731253904720111,\n"," (9, 7): 3.564719694746112,\n"," (7, 1): -0.6871827903040341,\n"," (4, 5): -0.7937769294630936,\n"," (9, 3): -0.8213461008344954,\n"," (1, 4): 0.06327626780501514,\n"," (3, 9): -0.07872276426135359,\n"," (2, 3): -0.19107364690374662,\n"," (1, 9): 0.19495913077670843,\n"," (7, 5): -0.6953396714497672,\n"," (8, 7): 3.0080465612404885,\n"," (6, 5): -0.7324958860867973,\n"," (3, 5): -0.8189455165260605,\n"," (2, 7): 0.8367270513914241,\n"," (5, 3): -0.4596309288122232,\n"," (7, 9): -0.4523981435469975,\n"," (9, 2): -0.8431774568102992,\n"," (3, 1): -0.6830506441853609,\n"," (5, 7): 1.7132429935902884,\n"," (9, 9): 5.0,\n"," (7, 4): -0.6437352855643286,\n"," (1, 8): 0.37914151714702293,\n"," (4, 3): -0.37634232586787497,\n"," (1, 7): 0.570688950087372,\n"," (9, 5): -0.7682284421540334,\n"," (5, 2): -0.525529612393064}"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"izMwRtifbaHp","outputId":"5188c20f-5fc1-4296-867e-c47d03858587"},"source":["maze.to_grid(uu)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[None, None, None, None, None, None, None, None, None, None, None],\n"," [None, 0.19, 0.05, -0.08, -0.19, -0.29, -0.38, -0.45, None, 5.0, None],\n"," [None, 0.38, None, None, None, None, None, None, None, 4.27, None],\n"," [None, 0.57, 0.84, 1.09, 1.38, 1.71, 2.09, 2.52, 3.01, 3.56, None],\n"," [None, 0.38, None, None, None, None, None, None, None, None, None],\n"," [None, 0.21, None, -0.82, -0.79, -0.77, -0.73, -0.7, -0.73, -0.77, None],\n"," [None, 0.06, None, None, None, None, None, -0.64, None, -0.8, None],\n"," [None, -0.08, -0.19, -0.29, -0.38, -0.46, -0.53, -0.59, None, -0.82, None],\n"," [None, None, None, None, None, -0.53, None, -0.64, None, -0.84, None],\n"," [None, -6.73, -0.72, -0.68, -0.64, -0.59, None, -0.69, None, -0.86, None],\n"," [None, None, None, None, None, None, None, None, None, None, None]]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"yPNRCCKSbaHs"},"source":["pi = best_policy(maze, uu)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGR-S6HTbaHv"},"source":["def isnumber(x):\n","    \"\"\"Is x a number?\"\"\"\n","    return hasattr(x, '__int__')\n","    \n","def print_table(table, header=None, sep='   ', numfmt='{}'):\n","    \"\"\"Print a list of lists as a table, so that columns line up nicely.\n","    header, if specified, will be printed as the first row.\n","    numfmt is the format for all numbers; you might want e.g. '{:.2f}'.\n","    (If you want different formats in different columns,\n","    don't use print_table.) sep is the separator between columns.\"\"\"\n","    justs = ['rjust' if isnumber(x) else 'ljust' for x in table[0]]\n","\n","    if header:\n","        table.insert(0, header)\n","\n","    table = [[numfmt.format(x) if isnumber(x) else x for x in row]\n","             for row in table]\n","\n","    sizes = list(map(lambda seq: max(map(len, seq)), list(zip(*[map(str, row) for row in table]))))\n","\n","    for row in table:\n","        print(sep.join(getattr(str(x), j)(size) for (j, size, x) in zip(justs, sizes, row)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vqY1g1qSa1G3"},"source":["Let's print out the best policy:"]},{"cell_type":"code","metadata":{"id":"i99AtsPda3BG","outputId":"f17cec15-8075-4075-e5e3-13a9f59bc383"},"source":["print_table(maze.to_arrows(pi))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None   None   None   None   None   None   None   None   None   None   None\n","None   v      <      <      <      <      <      <      None   .      None\n","None   v      None   None   None   None   None   None   None   ^      None\n","None   >      >      >      >      >      >      >      >      ^      None\n","None   ^      None   None   None   None   None   None   None   None   None\n","None   ^      None   >      >      >      >      v      <      <      None\n","None   ^      None   None   None   None   None   v      None   ^      None\n","None   ^      <      <      <      <      <      <      None   ^      None\n","None   None   None   None   None   ^      None   ^      None   ^      None\n","None   >      >      >      >      ^      None   ^      None   ^      None\n","None   None   None   None   None   None   None   None   None   None   None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6Zn3881stPnH"},"source":["This is the heatmap generated by the GridMDP editor using `value_iteration` on this environment.\n","Se observa como la política encontrada siempre intenta llevarnos hacia el punto con mayor recompensa.\n","<br>\n","![title](https://github.com/aimacode/aima-python/blob/master/images/mdp-d.png?raw=1)\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"aETbKtby7uiz"},"source":["### Solving the example from Fig. 17.1"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"329FSEk_7yly","outputId":"3500a5c2-d08f-4ea5-9ca8-8f475dd5fbf6"},"source":["sequential_decision_environment"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.GridMDP at 0x274e75d4710>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"rFD4RttQ70Gu"},"source":["u_171 = value_iteration(sequential_decision_environment)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"8Ds0pCIG8r8s","outputId":"3dfe25b8-285b-41bf-bd8d-d07fc18111a4"},"source":["u_171"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(0, 1): 0.3984432178350045,\n"," (1, 2): 0.649585681261095,\n"," (3, 2): 1.0,\n"," (0, 0): 0.2962883154554812,\n"," (3, 0): 0.12987274656746342,\n"," (3, 1): -1.0,\n"," (2, 1): 0.48644001739269643,\n"," (2, 0): 0.3447542300124158,\n"," (2, 2): 0.7953620878466678,\n"," (1, 0): 0.25386699846479516,\n"," (0, 2): 0.5093943765842497}"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"oADI63RJ8xIr"},"source":["pi_171 = best_policy(sequential_decision_environment, u_171)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"GURBye6e8yz0","outputId":"b97df271-3a4c-42aa-daae-9bcc62468211"},"source":["print_table(sequential_decision_environment.to_arrows(pi_171))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[">   >      >   .\n","^   None   ^   .\n","^   >      ^   <\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zhzc0r6Jaf3y"},"source":["### Ejemplo más sencillo"]},{"cell_type":"markdown","metadata":{"id":"iYt8FfKRb7v7"},"source":["Usando una [GUI](https://github.com/aimacode/aima-python/blob/master/gui/grid_mdp.py) disponible en el repositorio AIMA, podemos generar un entorno MDP del tipo grilla y visualizar en un *heatmap* los valores encontrados luego de las iteraciones."]},{"cell_type":"code","metadata":{"id":"5JxPsNBktPm9"},"source":["gridXD = [\n","    [None, None, None, None, None, None, None],\n","    [None, -0.1, -0.1, -0.1, -0.1, +5.0, None],\n","    [None, -0.1, None, None, None, None, None],\n","    [None, -0.1, -0.1, -0.1, -0.1, -0.1, None],\n","    [None, None, -0.1, None, -0.1, None, None],\n","    [None, -5.0, -0.1, -0.1, -0.1, -0.1, None],\n","    [None, None, None, None, None, None, None]\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPZksY49tPm_"},"source":["terminals = [(len(gridXD)-2, len(gridXD)-2)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwnXGYK6tPnB"},"source":["We define our maze environment below"]},{"cell_type":"code","metadata":{"id":"TH-R6XVMtPnB"},"source":["maze = GridMDP(gridXD, terminals)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wBDvFaH8tPnF"},"source":["To solve the maze, we can use the `best_policy` function along with `value_iteration`."]},{"cell_type":"code","metadata":{"id":"rvHnHShvLMBR","outputId":"05d15098-ed3d-488d-e90f-b414e1db680c"},"source":["maze"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.GridMDP at 0x274e75d4668>"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"Ejr4fg6OARyj"},"source":["uu = value_iteration(maze)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pLwHBNf1AVlF","outputId":"14088872-61d6-4804-f3df-5b52967e6836"},"source":["uu"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(5, 1): 0.16208115706883838,\n"," (1, 3): 1.6740473762040475,\n"," (3, 3): 1.0343790365962378,\n"," (5, 5): 5.0,\n"," (4, 5): 4.2682926829268295,\n"," (3, 1): 0.16208115706900714,\n"," (2, 1): 0.12059623985102505,\n"," (1, 4): 2.090082298035416,\n"," (1, 5): 2.5192603951702575,\n"," (2, 3): 1.316932007524478,\n"," (4, 3): 0.7626891689382147,\n"," (2, 2): 1.0343790365962378,\n"," (5, 3): 0.5477243211569478,\n"," (4, 2): 0.5477243211569478,\n"," (2, 5): 3.0616938233629822,\n"," (4, 1): 0.3235224367984536,\n"," (1, 1): -5.991793935750375,\n"," (3, 5): 3.625817965496729}"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"Fl-bZpH-BCJA","outputId":"aa808d24-72f7-4faf-bf49-d879aa32e613"},"source":["maze.to_grid(uu)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[None, None, None, None, None, None, None],\n"," [None, 2.52, 3.06, 3.63, 4.27, 5.0, None],\n"," [None, 2.09, None, None, None, None, None],\n"," [None, 1.67, 1.32, 1.03, 0.76, 0.55, None],\n"," [None, None, 1.03, None, 0.55, None, None],\n"," [None, -5.99, 0.12, 0.16, 0.32, 0.16, None],\n"," [None, None, None, None, None, None, None]]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"yCQLH7rPtPnF"},"source":["pi = best_policy(maze, uu)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lapVuJsyuObU","outputId":"b526e2ce-a3aa-442b-f257-27d10acbe1dc"},"source":["pi"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{(5, 1): (-1, 0),\n"," (1, 3): (0, 1),\n"," (3, 3): (-1, 0),\n"," (5, 5): None,\n"," (4, 5): (1, 0),\n"," (3, 1): (1, 0),\n"," (2, 1): (1, 0),\n"," (1, 4): (0, 1),\n"," (1, 5): (1, 0),\n"," (2, 3): (-1, 0),\n"," (4, 3): (-1, 0),\n"," (2, 2): (0, 1),\n"," (5, 3): (-1, 0),\n"," (4, 2): (0, 1),\n"," (2, 5): (1, 0),\n"," (4, 1): (0, 1),\n"," (1, 1): (1, 0),\n"," (3, 5): (1, 0)}"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"F04E8Z4_tPnI","outputId":"fd4bc69b-8b95-4d9e-e42a-fc0edaa91a33"},"source":["print_table(maze.to_arrows(pi))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None   None   None   None   None   None   None\n","None   >      >      >      >      .      None\n","None   ^      None   None   None   None   None\n","None   ^      <      <      <      <      None\n","None   None   ^      None   ^      None   None\n","None   >      >      >      ^      <      None\n","None   None   None   None   None   None   None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tmFaGZSmtPnJ"},"source":["As you can infer, we can find the path to the terminal state starting from any given state using this policy.\n","All maze problems can be solved by formulating it as a MDP."]},{"cell_type":"markdown","metadata":{"id":"VN5bu3Yc3Af0"},"source":["Con el programa Grid MDP, se puede construir manualmente el entorno, para resolverlo gráficamente:\n","\n","<img src='https://raw.githubusercontent.com/iapucp/IA-Diplomado/master/Inteligencia%20Artificial%20para%20Juegos/Codigo_Sesion6/gridMDP_values_manual.png' width=500px>"]},{"cell_type":"markdown","metadata":{"id":"fi3CkINc2nnd"},"source":["Ejecutando el programa auxiliar Grid MDP, se genera el siguiente heatmap:\n","\n","<img src='https://raw.githubusercontent.com/iapucp/IA-Diplomado/master/Inteligencia%20Artificial%20para%20Juegos/Codigo_Sesion6/gridMDP_colors.png' width=500px>"]},{"cell_type":"markdown","metadata":{"id":"0kXl709e20iI"},"source":["Luego de resolver el entorno, se consiguen los siguientes valores:\n","\n","<img src='https://raw.githubusercontent.com/iapucp/IA-Diplomado/master/Inteligencia%20Artificial%20para%20Juegos/Codigo_Sesion6/gridMDP_value_tables.png' width=200px>\n","\n"]}]}